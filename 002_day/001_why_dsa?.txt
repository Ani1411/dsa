Why Learn Data Structures and Algorithms?

Programming is all about data structures and algorithms.
Data structures are used to hold data while algorithms are used to solve the problem using that data.

Data structures and algorithms (DSA) goes through solutions to standard problems in detail and
gives you an insight into how efficient it is to use each one of them.
It also teaches you the science of evaluating the efficiency of an algorithm.
This enables you to choose the best of various choices.


Use of Data Structures and Algorithms to Make Your Code Scalable

Time is precious.

Suppose, Alice and Bob are trying to solve a simple problem of finding the sum of the first 10Ë†11 natural numbers.
While Bob was writing the algorithm, Alice implemented it proving that it is as simple as criticizing Donald Trump.

Algorithm (by Bob)

Initialize sum = 0
for every natural number n in range 1 to 1011(inclusive):
    add n to sum
sum is your answer


Code (by Alice)

def findSum():  # F:001.py LN: 1
    sum = 0;
    v = 1
    while v < 100000000000:
        sum += v;
        v += 1
    return sum;


A computer is the most deterministic machine.
Going back and trying to run it again won't help. So let's analyze what's wrong with this simple code.

Two of the most valuable resources for a computer program are TIME and MEMORY.
The time taken by the computer to run code is:
    Time to run code = number of instructions * time to execute each instruction


In this case,
    the total number of instructions executed:
    (let's say x) are x = 1 + (1011 + 1) + (1011) + 1,
        which is x = 2 * 1011 + 3
    Let us assume that a computer can execute y = 108 instructions in one second (it can vary subject to machine configuration).
    The time taken to run above code is
        Time to run y instructions = 1 second
        Time to run 1 instruction = 1 / y seconds
        Time to run x instructions = x * (1/y) seconds = x / y seconds

    Hence,
    Time to run the code = x / y
                         = (2 * 1011 + 3) / 108 (greater than 33 minutes)

OPTIMISED
The sum of first N natural numbers is given by the formula:

Sum = N * (N + 1) / 2

Converting it into code will look something like this:

fn sum(N) {
    return N * (N + 1) / 2;
}

If you see our first solution to find the sum of first N natural numbers, it wasn't scalable.
It's because it required linear growth in time with the linear growth in the size of the problem.
Such algorithms are also known as LINEARLY SCALABLE ALGORITHMS.

Our second solution was very scalable and didn't require the use of any more time to solve a problem of larger size.
These are known as CONSTANT-TIME ALGORITHMS.

MEMORY IS EXPENSIVE
Memory is not always available in abundance. While dealing with code/system which requires you to store or produce a lot of data, it is critical for your algorithm to save the usage of memory wherever possible. For example: While storing data about people, you can save memory by storing only their date of birth, not their age. You can always calculate it on the fly using their date of birth and current date.


Example 1: Age Group Problem
Problems like finding the people of a certain age group can easily be solved,
    with a little modified version of the binary search algorithm (assuming that the data is sorted).

The naive algorithm which goes through all the persons one by one,
    and checks if it falls in the given age group is linearly scalable.
    Whereas, binary search claims itself to be a LOGARITHMICALLY SCALABLE ALGORITHM.
    This means that if the size of the problem is squared, the time taken to solve it is only doubled.

Suppose, it takes 1 second to find all the people at a certain age for a group of 1000. Then for a group of 1 million people,
    * the binary search algorithm will take only 2 seconds to solve the problem.
    * the naive algorithm might take 1 million seconds, which is around 12 days.
    The same binary search algorithm is used to find the square root of a number.

Example 2: Rubik's Cube Problem
Imagine you are writing a program to find the solution of a Rubik's cube.
This cute looking puzzle has annoyingly 43,252,003,274,489,856,000 positions, and these are just positions!
Imagine the number of paths one can take to reach the wrong positions.

Fortunately, the way to solve this problem can be represented by the graph data structure.
There is a graph algorithm known as Dijkstra's algorithm which allows you to solve this problem in linear time.
Yes, you heard it right. It means that it allows you to reach the solved position in a minimum number of states.

Example 3: DNA Problem
DNA is a molecule that carries genetic information.
They are made up of smaller units which are represented by Roman characters A, C, T, and G.
Imagine yourself working in the field of bioinformatics.
You are assigned the work of finding out the occurrence of a particular pattern in a DNA strand.

It is a famous problem in computer science academia.
And, the simplest algorithm takes the time proportional to
    (number of character in DNA strand) * (number of characters in pattern)

A typical DNA strand has millions of such units.
    Eh! worry not. KMP algorithm can get this done in time which is proportional to
    (number of character in DNA strand) + (number of characters in pattern)
The * operator replaced by + makes a lot of change.

Considering that the pattern was of 100 characters, your algorithm is now 100 times faster.
If your pattern was of 1000 characters, the KMP algorithm would be almost 1000 times faster.
That is, if you were able to find the occurrence of pattern in 1 second, it will now take you just 1 ms.
We can also put this in another way.
    Instead of matching 1 strand, you can match 1000 strands of similar length at the same time.
